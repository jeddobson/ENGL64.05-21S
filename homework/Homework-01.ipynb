{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Cultural Analytics: Homework #1</h1></center>\n",
    "<center><h2>ENGL 64.05, 21S</h2></center>\n",
    "<br>\n",
    "<center><b>Due</b> 11:59PM 4/9/2021</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lines tell the Python interpreter to load some additional functions.\n",
    "# We need functions to parse Comma Separated Value (CSV) data, calculate\n",
    "# means and load a local library for reading term frequency data\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('shared/engl64.05/lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Title Length Analytics\n",
    "# HW1.1\n",
    "#####################################################\n",
    "\n",
    "# we'll set a variable to hold the number of records read from this Comma Separated Value (CSV) file.\n",
    "ln = 0\n",
    "\n",
    "# as above, we'll create a metadata variable (overwritting the previous variable)\n",
    "metadata=list()\n",
    "\n",
    "# Now we open the CSV file and read each line, appending to the metadata list:\n",
    "with open('shared/engl64.05/data/Underwood_ch1/allgenremeta.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        metadata.append(row)\n",
    "        \n",
    "        # increment our counter\n",
    "        ln += 1\n",
    "\n",
    "# tell us how many entries we've read\n",
    "print(\"read {0} lines\".format(ln))\n",
    "\n",
    "# Display header (our \"metadata\" for our metadata)\n",
    "print(metadata[0])\n",
    "\n",
    "# Display index for ease in processing\n",
    "print([x for x in range(len(metadata[0]))])\n",
    "\n",
    "# And now let's remove the header for easier processing:\n",
    "metadata = metadata[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.1: Title Length Change over Time\n",
    "\n",
    "In the cell below, determine if title length changes over time within this dataset. There are several different ways you could produce this data.\n",
    "\n",
    "<b>Hint:</b>\n",
    "- If you wish, you can iterate through a list sorted by publication date by operating on \n",
    "<pre>sorted(metadata, key=lambda book: book[2])</pre>\n",
    "- You can determine the mean (average) of a list of values with the following imported function:\n",
    "<pre>np.mean(list_name)</pre> \n",
    "- As the publication years are simply stored as strings after reading from the CSV file, you can match decades by comparing the first three characters of a string: <pre>[len(book[10]) for book in metadata if book[2].startswith('186')]</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #1: Written response</b> (Don't forget to save your notebook before closing.) In no more than two-hundred words describe what you have discovered, if it aligns with Moretti's claims, and what critical questions you might ask about the data and method.\n",
    "\n",
    "Response goes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Entropic Poem\n",
    "# HW1.2\n",
    "#####################################################\n",
    "\n",
    "# cut-and-paste some text into the space between the \"\"\" markers\n",
    "source_text = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# preprocess using the Natural Language Toolkit (NLTK) Tokenizer\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(source_text)\n",
    "\n",
    "# drop to lowercase\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# create our vocabulary \n",
    "vocab = dict()\n",
    "\n",
    "# now produce the entropic poem\n",
    "for word in tokens:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = tokens.count(word)\n",
    "        print(vocab[word],word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.2: Entropic Poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #2:</b> In no more than two hundred words respond the following prompt: Cut-and-paste some text into the space between the \"\"\" markers above and run the cell. Why did you select the text that you selected? What did you notice about your \"entropic poem?\" Does it work better with certain kinds of texts? \n",
    "\n",
    "Response goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Type/Token Ratios\n",
    "# HW1.3\n",
    "#####################################################\n",
    "\n",
    "from tsvdro import tsvdro\n",
    "\n",
    "document = tsvdro.load(\"document_goes_here\")\n",
    "\n",
    "\n",
    "tokens = sum([int(x) for x in document['data'].values()])\n",
    "types = len(document['data'])\n",
    "\n",
    "print(\"{0:50s} {1:20s} {2:5s} {3:3s}\".format(\"Title\",\"Author\",\"Genre\",\"TTR\"))\n",
    "print(\"{0:50s} {1:20s} {2:5s} {3:f}\".format(document['header']['bibliographic_data']['title'],document['header']['bibliographic_data']['author_name'],\n",
    "document['header']['bibliographic_data']['genre'],round(types/tokens*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.3: Type/Token Ratios\n",
    "\n",
    "Select some texts (four or five) from Ted Underwood's dataset and calcuate the type/token ratio. The above code is complete with the exception of the filename. The filenames are stored in the first element of the metadata list. You can easily obtain some filenames with:\n",
    "<pre>\n",
    "print([book[0] for book in metadata[5100:5110]])\n",
    "</pre>\n",
    "You'll have to include the directory name in which these files are found and append \".dro\" to each file name. For example, \"nyp.33433082246376\" will be:\n",
    "<pre>\n",
    "shared/engl64.05/data/Underwood_ch1/nyp.33433082246376.dro\n",
    "</pre>\n",
    "\n",
    "After calculating the ratio, take a quick look at the contents of the following variable:\n",
    "<pre>document['data']</pre>\n",
    "before moving on to the next file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #3:</b> In no more than two hundred words respond the following prompt: What surprises you about these results? What questions do you have about the dataset? What information do you need to answer these questions?\n",
    "\n",
    "Response goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
